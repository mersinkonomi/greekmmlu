# GreekMMLU

### Paper

Title: GreekMMLU: Assessing Massive Multitask Language Understanding in Greek

Abstract: 
The evaluation of Large Language Models (LLMs) has primarily centered on high-resource languages, often leaving languages like Greek with insufficient or low-quality assessment tools. Existing Greek benchmarks frequently rely on machine-translated versions of English datasets, which fail to capture the authentic linguistic nuances and cultural context of the language. In this paper, we introduce GreekMMLU, a comprehensive benchmark consisting of 19,731 test samples and 145 development samples across 29 diverse subjects. Unlike previous efforts, our dataset is curated from authentic, native Greek sources, including academic and professional examinations. We detail our rigorous cleaning and filtering pipeline designed to ensure data integrity. Our results indicate that while current LLMs show basic proficiency, they struggle with the complexity of native-sourced Greek content, underscoring the necessity for high-fidelity, non-translated evaluation frameworks.




### Citation

```

```


### Groups and Tasks

#### Groups

* `gmmlu`: evaluates all GreekMMLU tasks.

* `gmmlu_stem`: evaluates STEM GreekMMLU tasks.
* `gmmlu_social_sciences`: evaluates social science GreekMMLU tasks.
* `gmmlu_humanities`: evaluates humanities GreekMMLU tasks.
* `gmmlu_other`: evaluates other GreekMMLU tasks.
